{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a2b469",
   "metadata": {},
   "source": [
    "### This will include more analysis on top of previous Srcasm Detection.\n",
    "### Which will inlcude Word3vec,MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5eff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d49cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3dd830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a703601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5087ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='train-balanced-sarcasm.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa933a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c977c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(axis=0,subset=['comment'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0577eaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: comment, dtype: object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.isnull().any(axis=1)]['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df998af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(row):\n",
    "    row = row.lower()\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    words = row.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words) if words else 'empty'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e7b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X_train,train_data=False):\n",
    "    X=[]\n",
    "    for row in X_train:\n",
    "        row=preprocess(row)\n",
    "        X.append(row)\n",
    "    if(train_data==True):\n",
    "        X_tfidf=tfidf.fit_transform(X)\n",
    "    else:\n",
    "        X_tfidf=tfidf.transform(X)\n",
    "    return X_tfidf\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00cedc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train,y_train,model):\n",
    "    X=get_features(X_train,train_data=True)\n",
    "    model.fit(X,y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407fb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_pred=train_test_split(train_df['comment'],train_df['label'],test_size=0.3,train_size=0.7)\n",
    "tfidf = TfidfVectorizer(stop_words='english',ngram_range=(1, 2))\n",
    "LR_model=LogisticRegression(random_state=42,max_iter=1000)\n",
    "model=train_model(X_train,y_train,LR_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ecaec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(trained_model,X_test,y_test):\n",
    "    X=get_features(X_test,train_data=False)\n",
    "    predictions=trained_model.predict(X)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    cm=confusion_matrix(y_test,predictions)\n",
    "    # Print metrics for analysis\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45c78c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Precision: 0.69\n",
      "Recall: 0.67\n",
      "F1 Score: 0.68\n",
      "[[105114  46295]\n",
      " [ 49653 102170]]\n"
     ]
    }
   ],
   "source": [
    "model_predict(model,X_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3e73d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,X_test,y_train,y_pred=train_test_split(train_df['comment'],train_df['label'],test_size=0.3,train_size=0.7)\n",
    "#tfidf = TfidfVectorizer(stop_words='english')\n",
    "nb=MultinomialNB()\n",
    "model=train_model(X_train,y_train,nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a8cfa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Precision: 0.67\n",
      "Recall: 0.69\n",
      "F1 Score: 0.68\n",
      "[[ 99964  51445]\n",
      " [ 46703 105120]]\n"
     ]
    }
   ],
   "source": [
    "model_predict(model,X_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f9906",
   "metadata": {},
   "source": [
    "## Working On LR with Word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47f5d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28012c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca0e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(row):\n",
    "    row = row.lower()\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    words = row.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words) if words else 'empty'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5af6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_with_word2vec(texts):\n",
    "    training_vectors=[]\n",
    "    for row in texts:\n",
    "        row = preprocess(row)  # Preprocessing remains consistent\n",
    "        words = row.split(\" \")\n",
    "        vector_list = []\n",
    "        for word in words:\n",
    "            if word in word2Vec:  # Ensure word is in the GloVe model\n",
    "                vector_list.append(word2Vec[word])\n",
    "        if vector_list:\n",
    "            training_vectors.append(np.max(vector_list, axis=0))  # Simple average across vectors\n",
    "        else:\n",
    "            training_vectors.append(np.zeros(word2Vec.vector_size))# Return zeros if no vectors found\n",
    "    return training_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57e67dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR_with_word2vec(X_train,y_train,LR_model):\n",
    "    return LR_model.fit(X_train,y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bda68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train_df['comment'],train_df['label'],test_size=0.2,train_size=0.8)\n",
    "LR_model=LogisticRegression(random_state=42,max_iter=1000,class_weight='balanced',C=1)\n",
    "training_vectors=create_vector_with_word2vec(X_train)\n",
    "model=train_LR_with_word2vec(training_vectors,y_train,LR_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f577ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(trained_model,X_test,y_test):\n",
    "    def get_text_vector(text):\n",
    "        text = preprocess(text)\n",
    "        words = text.split(\" \")\n",
    "        vector_list = []\n",
    "        for word in words:\n",
    "            if word in word2Vec:\n",
    "                vector_list.append(word2Vec[word])\n",
    "        if vector_list:\n",
    "            return np.mean(vector_list, axis=0)\n",
    "        else:\n",
    "            return np.zeros(word2Vec.vector_size)  # Zero vector if no valid words\n",
    "    X_test_vec=[]\n",
    "    for row in X_test:\n",
    "        X_test_vec.append(get_text_vector(row))\n",
    "    predictions=trained_model.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    cm=confusion_matrix(y_test,predictions)\n",
    "    # Print metrics for analysis\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(cm)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()\n",
    "    print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "925a79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Precision: 0.69\n",
      "Recall: 0.21\n",
      "F1 Score: 0.32\n",
      "[[91212  9555]\n",
      " [80129 21259]]\n",
      "91212 9555 80129 21259\n"
     ]
    }
   ],
   "source": [
    "model_predict(model,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bec1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae851435",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(row):\n",
    "    row = row.lower()\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    words = row.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words) if words else 'empty'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c4e466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the average vector for a set of reviews\n",
    "def get_average_vector(texts,label):    \n",
    "    training_vectors=[]\n",
    "    count_label=[]\n",
    "    for row in texts:\n",
    "        row = preprocess(row)  # Preprocessing remains consistent\n",
    "        words = row.split(\" \")\n",
    "        vector_list = []\n",
    "        for word in words:\n",
    "            if word in word2Vec:  # Ensure word is in the GloVe model\n",
    "                vector_list.append(word2Vec[word])\n",
    "        if vector_list:\n",
    "            training_vectors.append(np.mean(vector_list, axis=0))  # Simple average across vectors\n",
    "        else:\n",
    "            training_vectors.append(np.zeros(word2Vec.vector_size))# Return zeros if no vectors found\n",
    "        count_label.append(label)\n",
    "    return training_vectors,count_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f837c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_model_average(train_df):\n",
    "    #word2Vec = gensim.downloader.load('glove-twitter-25')\n",
    "    positive_labels=train_df[train_df['label']==1]['comment']\n",
    "    negative_labels=train_df[train_df['label']==0]['comment']\n",
    "    positive_vector,positive_label=get_average_vector(positive_labels,1)\n",
    "    negative_vector,negative_label=get_average_vector(negative_labels,0)\n",
    "    X_train=positive_vector+negative_vector\n",
    "    y_train=positive_label+negative_label\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=500, random_state=42)\n",
    "    # Convert lists to numpy arrays for model training\n",
    "    print(len(X_train),len(y_train))\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d95245b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48205b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808616 808616\n"
     ]
    }
   ],
   "source": [
    "mlp=train_MLP_model_average(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e06283db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_MLP_model(test, MLP_model, input_type='average'):\n",
    "    '''df_test = pd.read_csv(path_to_test_file, sep='\\t', header=None)\n",
    "    df_test = df_test.rename(columns={0: 'text', 1: 'label'})'''\n",
    "\n",
    "    # Helper function to choose between average and max pooling\n",
    "    def get_text_vector(text):\n",
    "        text = preprocess(text)\n",
    "        words = text.split(\" \")\n",
    "        vector_list = []\n",
    "        for word in words:\n",
    "            if word in word2Vec:\n",
    "                vector_list.append(word2Vec[word])\n",
    "        if vector_list:\n",
    "            if input_type == 'average':\n",
    "                return np.mean(vector_list, axis=0)  # Average vector\n",
    "        else:\n",
    "            return np.zeros(word2Vec.vector_size)  # Zero vector if no valid words\n",
    "\n",
    "    # Create vectors for the test set\n",
    "    X_test = np.array([get_text_vector(row) for row in test['comment']])\n",
    "    y_test = test['label'].values\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    y_pred = MLP_model.predict(X_test)\n",
    "    y_prob = MLP_model.predict_proba(X_test)[:, 1]  # Probability of being positive\n",
    "\n",
    "    # Add new columns to the test dataframe\n",
    "    test['probability_positive'] = y_prob\n",
    "    test['predicted_class'] = y_pred\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c230465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.650495906606317\n",
      "Precision: 0.6635338547533983\n",
      "Recall: 0.6114950711397186\n",
      "F1 Score: 0.6364525125292005\n",
      "Confusion Matrix:\n",
      "[[69655 31361]\n",
      " [39293 61846]]\n"
     ]
    }
   ],
   "source": [
    "test_MLP_model(test,mlp) #twitter 200 without smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38d380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
